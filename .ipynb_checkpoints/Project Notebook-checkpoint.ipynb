{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import ImageGrab\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import io\n",
    "import time\n",
    "%matplotlib inline \n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (15, 9)\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "game_url = \"game/dino.html\"\n",
    "chrome_driver_path = \"../chromedriver.exe\"\n",
    "class Game:\n",
    "    def __init__(self,custom_config=True):\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"disable-infobars\")\n",
    "        self._driver = webdriver.Chrome(executable_path = chrome_driver_path,chrome_options=chrome_options)\n",
    "        self._driver.set_window_position(x=-10,y=0)\n",
    "        self._driver.set_window_size(200, 300)\n",
    "        self._driver.get(os.path.abspath(game_url))\n",
    "        if custom_config:\n",
    "            self._driver.execute_script(\"Runner.config.ACCELERATION=0\")\n",
    "    def get_crashed(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.crashed\")\n",
    "    def get_playing(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.playing\")\n",
    "    def restart(self):\n",
    "#         self._driver.execute_script(\"Runner.instance_.config.CLEAR_TIME = \"+ str(randint(1, 999)))\n",
    "        self._driver.execute_script(\"Runner.instance_.restart()\")\n",
    "#         self._driver.refresh()\n",
    "#         self.press_up()\n",
    "        time.sleep(0.25)\n",
    "    def press_up(self):\n",
    "        self._driver.find_element_by_tag_name(\"body\").send_keys(Keys.ARROW_UP)\n",
    "    def press_down(self):\n",
    "        self._driver.find_element_by_tag_name(\"body\").send_keys(Keys.ARROW_DOWN)\n",
    "    def get_score(self):\n",
    "        score_array = self._driver.execute_script(\"return Runner.instance_.distanceMeter.digits\")\n",
    "        score = ''.join(score_array)\n",
    "        return int(score)\n",
    "    def pause(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.stop()\")\n",
    "    def resume(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.play()\")\n",
    "    def end(self):\n",
    "        self._driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DinoAgent:\n",
    "    def __init__(self,game):\n",
    "        self._game = game;\n",
    "        self.jump();\n",
    "        time.sleep(.5)\n",
    "    def is_running(self):\n",
    "        return self._game.get_playing()\n",
    "    def is_crashed(self):\n",
    "        return self._game.get_crashed()\n",
    "    def jump(self):\n",
    "        self._game.press_up()\n",
    "    def duck(self):\n",
    "        self._game.press_down()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#processing image as required\n",
    "def process_img(image):\n",
    "    #game is already in grey scale canvas, canny to get only edges and reduce unwanted objects(clouds)\n",
    "#     image = cv2.Canny(image, threshold1 = 100, threshold2 = 200)\n",
    "#     image = image[10:140,0:200] #img[y:y+h, x:x+w]\n",
    "#     image = resized_image = cv2.resize(image, (80, 80)) \n",
    "    image = cv2.resize(image, (0,0), fx = 0.15, fy = 0.10)\n",
    "    image = image[2:38,10:50] #img[y:y+h, x:x+w]\n",
    "    image = cv2.Canny(image, threshold1 = 100, threshold2 = 200)\n",
    "    return  image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grab_screen(_driver = None):\n",
    "    screen =  np.array(ImageGrab.grab(bbox=(40,180,440,400)))\n",
    "    image = process_img(screen)\n",
    "    if _driver!=None:\n",
    "        image = _driver.get_screenshot_as_png()\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # print(grab_screen().shape)\n",
    "# # game = Game()\n",
    "# # dino = DinoAgent(game)\n",
    "# # last_time = time.time()\n",
    "# while(True):\n",
    "    \n",
    "# # # #     print('loop took {} seconds'.format(time.time()-last_time))\n",
    "# # # #     last_time = time.time()\n",
    "# # # #     cv2.imwrite(\"./img_data/dino\"+str(time())+\".jpg\",image)\n",
    "# # # #     dino.duck()\n",
    "# # #     #exit on q pres\n",
    "# # # #     print('{0} {1} '.format(r_t,end_t))\n",
    "# # # #     cv2.imshow('window',game.grab_screen())\n",
    "#     image = grab_screen()\n",
    "#     cv2.imshow('window',image)\n",
    "    \n",
    "\n",
    "# # # #     from matplotlib import pyplot as plt\n",
    "# # # #     plt.imshow(image)\n",
    "# # # #     plt.title('my picture')\n",
    "# # # #     plt.show()\n",
    "\n",
    "# # # #     grab_screen()\n",
    "# # #     if(dino.is_crashed()):\n",
    "# # #         #jumping starts the game again if dino has crashed\n",
    "# # # #         temp = (game.get_score())\n",
    "# # #         game.restart()\n",
    "#     if (cv2.waitKey(25) & 0xFF == ord('q')):\n",
    "#         cv2.destroyAllWindows()\n",
    "# #         game.end()\n",
    "# #         cv2.imwrite('dino.jpg',image)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_obj(obj, name ):\n",
    "    with open('objects/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('objects/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#game parameters\n",
    "ACTIONS = 2 # possible actions: jump, do nothing\n",
    "GAMMA = 0.8 # decay rate of past observations original 0.99\n",
    "OBSERVATION = 20000. # timesteps to observe before training\n",
    "EXPLORE = 50000 #300000. # frames over which to anneal epsilon\n",
    "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
    "INITIAL_EPSILON = 0.1 # starting value of epsilon\n",
    "REPLAY_MEMORY = 50000 # number of previous transitions to remember\n",
    "BATCH = 32 # size of minibatch\n",
    "FRAME_PER_ACTION = 1\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \"\"\"initial variable caching, done only once\"\"\"\n",
    "# save_obj(INITIAL_EPSILON,\"epsilon\")\n",
    "# t = 0\n",
    "# save_obj(t,\"time\")\n",
    "# # D = deque()\n",
    "# # save_obj(D,\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_rows , img_cols = 40,20\n",
    "img_channels = 4 #We stack 4 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildmodel():\n",
    "    print(\"Now we build the model\")\n",
    "    model = Sequential()\n",
    "#     Conv2D(32, (8, 8), strides=(4, 4), input_shape=(20, 40, 4..., padding=\"same\")`\n",
    "    model.add(Conv2D(32, (8, 8), strides=(4, 4), padding='same',input_shape=(img_cols,img_rows,img_channels)))  #80*80*4\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (4, 4), strides=(2, 2), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3), strides=(1, 1), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(ACTIONS))\n",
    "    adam = Adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='mse',optimizer=adam)\n",
    "    print(\"We finish building the model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainNetwork(model,game_state,observe=False):\n",
    "    # open up a game state to communicate with emulator\n",
    "    last_time = time.time()\n",
    "    # store the previous observations in replay memory\n",
    "    D = load_obj(\"D\") #deque()\n",
    "    # get the first state by doing nothing and preprocess the image to 80x80x4\n",
    "    do_nothing = np.zeros(ACTIONS)\n",
    "    do_nothing[0] =1 \n",
    "    x_t, r_0, terminal = game_state.get_state(do_nothing)\n",
    "    \n",
    "\n",
    "    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)\n",
    "    \n",
    "\n",
    "    #In Keras, need to reshape\n",
    "    s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])  #1*80*80*4\n",
    "    initial_state = s_t\n",
    "\n",
    "    if observe :#args['mode'] == 'Run':\n",
    "        OBSERVE = 999999999    #We keep observe, never train\n",
    "        epsilon = FINAL_EPSILON\n",
    "        print (\"Now we load weight\")\n",
    "        model.load_weights(\"model.h5\")\n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "        print (\"Weight load successfully\")    \n",
    "    else:                       #We go to training mode\n",
    "        OBSERVE = OBSERVATION\n",
    "        epsilon = load_obj(\"epsilon\")#FINAL_EPSILON #INITIAL_EPSILON\n",
    "        model.load_weights(\"model.h5\")\n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "\n",
    "    t = load_obj(\"time\")\n",
    "    while (True):\n",
    "        \n",
    "        loss = 0\n",
    "        Q_sa = 0\n",
    "        action_index = 0\n",
    "        r_t = 0\n",
    "        a_t = np.zeros([ACTIONS])\n",
    "        #choose an action epsilon greedy\n",
    "        if t % FRAME_PER_ACTION == 0:\n",
    "            if  random.random() <= epsilon:\n",
    "                print(\"----------Random Action----------\")\n",
    "                action_index = random.randrange(ACTIONS)\n",
    "                a_t[action_index] = 1\n",
    "            else:\n",
    "                q = model.predict(s_t)       #input a stack of 4 images, get the prediction\n",
    "                max_Q = np.argmax(q)\n",
    "                action_index = max_Q\n",
    "                a_t[action_index] = 1\n",
    "        #We reduced the epsilon gradually\n",
    "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
    "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE #update to original asap\n",
    "\n",
    "        #run the selected action and observed next state and reward\n",
    "        x_t1, r_t, terminal = game_state.get_state(a_t)\n",
    "        print('loop took {} seconds'.format(time.time()-last_time))\n",
    "        last_time = time.time()\n",
    "        x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1], 1) #1x80x80x1\n",
    "        s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3)\n",
    "        \n",
    "        \n",
    "        # store the transition in D\n",
    "        D.append((s_t, action_index, r_t, s_t1, terminal))\n",
    "        if len(D) > REPLAY_MEMORY:\n",
    "            D.popleft()\n",
    "\n",
    "        #only train if done observing\n",
    "        if t > OBSERVE: # and terminal: \n",
    "            \n",
    "            #sample a minibatch to train on\n",
    "            minibatch = random.sample(D, BATCH)\n",
    "            inputs = np.zeros((BATCH, s_t.shape[1], s_t.shape[2], s_t.shape[3]))   #32, 20, 40, 4\n",
    "            targets = np.zeros((inputs.shape[0], ACTIONS))                         #32, 2\n",
    "\n",
    "            #Now we do the experience replay\n",
    "            for i in range(0, len(minibatch)):\n",
    "                state_t = minibatch[i][0]\n",
    "                action_t = minibatch[i][1]   #This is action index\n",
    "                reward_t = minibatch[i][2]\n",
    "                state_t1 = minibatch[i][3]\n",
    "                terminal = minibatch[i][4]\n",
    "                # if terminated, only equals reward\n",
    "\n",
    "                inputs[i:i + 1] = state_t    #I saved down s_t\n",
    "\n",
    "                targets[i] = model.predict(state_t)  # Hitting each buttom probability\n",
    "                Q_sa = model.predict(state_t1)\n",
    "                \n",
    "                if terminal:\n",
    "                    targets[i, action_t] = reward_t\n",
    "                else:\n",
    "                    targets[i, action_t] = reward_t + GAMMA * np.max(Q_sa)\n",
    "            # targets2 = normalize(targets)\n",
    "            loss = model.train_on_batch(inputs, targets)\n",
    "            loss_df.loc[len(loss_df)] = loss\n",
    "        else:\n",
    "#             artificial time delay as training done with this delay\n",
    "#             loss_df.loc[len(loss_df)] = 0\n",
    "            time.sleep(0.15)\n",
    "        s_t = initial_state if terminal else s_t1\n",
    "        t = t + 1\n",
    "        \n",
    "        # save progress every 10000 iterations\n",
    "        if t % 1000 == 0:\n",
    "            print(\"Now we save model\")\n",
    "            \n",
    "            model.save_weights(\"model.h5\", overwrite=True)\n",
    "            save_obj(D,\"D\") #saving episodes\n",
    "            save_obj(t,\"time\") #caching time steps\n",
    "            save_obj(epsilon,\"epsilon\") #cache epsilon to avoid repeated randomness in actions\n",
    "            loss_df.to_csv(\"./objects/loss_df.csv\",index=False)\n",
    "            scores_df.to_csv(\"./objects/scores_df.csv\",index=False)\n",
    "            actions_df.to_csv(\"./objects/actions_df.csv\",index=False)\n",
    "            clear_output()\n",
    "            with open(\"model.json\", \"w\") as outfile:\n",
    "                json.dump(model.to_json(), outfile)\n",
    "\n",
    "        # print info\n",
    "        state = \"\"\n",
    "        if t <= OBSERVE:\n",
    "            state = \"observe\"\n",
    "        elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
    "            state = \"explore\"\n",
    "        else:\n",
    "            state = \"train\"\n",
    "\n",
    "        print(\"TIMESTEP\", t, \"/ STATE\", state, \\\n",
    "            \"/ EPSILON\", epsilon, \"/ ACTION\", action_index, \"/ REWARD\", r_t, \\\n",
    "            \"/ Q_MAX \" , np.max(Q_sa), \"/ Loss \", loss)\n",
    "\n",
    "    print(\"Episode finished!\")\n",
    "    print(\"************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_df = pd.read_csv(\"./objects/loss_df.csv\")\n",
    "scores_df = pd.read_csv(\"./objects/scores_df.csv\")\n",
    "actions_df = pd.read_csv(\"./objects/actions_df.csv\")\n",
    "q_max_df = pd.read_csv(\"./objects/q_values.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def playGame(observe=False):\n",
    "    game = Game()\n",
    "    dino = DinoAgent(game)\n",
    "    game_state = Game_sate(dino,game)\n",
    "    model = buildmodel()\n",
    "    trainNetwork(model,game_state,observe=observe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Game_sate:\n",
    "    def __init__(self,agent,game):\n",
    "        self._agent = agent\n",
    "        self._game = game\n",
    "        self._display = show_img()\n",
    "        self._display.__next__()\n",
    "    def get_state(self,actions):\n",
    "        actions_df.loc[len(actions_df)] = actions[1]\n",
    "        reward = 0.1\n",
    "        is_over = False\n",
    "        if actions[1] == 1:\n",
    "            self._agent.jump()\n",
    "        image = grab_screen()\n",
    "        self._display.send(image)\n",
    "\n",
    "        if self._agent.is_crashed():\n",
    "            score = self._game.get_score()\n",
    "            scores_df.loc[len(loss_df)] = score\n",
    "            self._game.restart()\n",
    "            reward = -11/score\n",
    "            is_over = True\n",
    "        return image, reward, is_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD , Adam\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import TensorBoard\n",
    "from collections import deque\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_img(graphs = False):\n",
    "    \"\"\"\n",
    "    Show images in new window\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        screen = (yield)\n",
    "        window_title = \"logs\" if graphs else \"game_play\"\n",
    "        cv2.namedWindow(window_title, cv2.WINDOW_NORMAL)        # Create window with freedom of dimensions\n",
    "        imS = cv2.resize(screen, (800, 400)) \n",
    "        cv2.imshow(window_title, screen)\n",
    "#         cv2.imwrite(\"screenshot\"+str(frame)+\".png\",screen)\n",
    "        if (cv2.waitKey(1) & 0xFF == ord('q')):\n",
    "            cv2.destroyAllWindows()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we build the model\n",
      "We finish building the model\n",
      "Now we load weight\n",
      "Weight load successfully\n",
      "loop took 3.917262077331543 seconds\n",
      "TIMESTEP 583001 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.21646928787231445 seconds\n",
      "TIMESTEP 583002 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.21997404098510742 seconds\n",
      "TIMESTEP 583003 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2337646484375 seconds\n",
      "TIMESTEP 583004 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23387408256530762 seconds\n",
      "TIMESTEP 583005 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24978256225585938 seconds\n",
      "TIMESTEP 583006 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2371079921722412 seconds\n",
      "TIMESTEP 583007 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.5208296775817871 seconds\n",
      "TIMESTEP 583008 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD -1.0 / Q_MAX  0 / Loss  0\n",
      "loop took 0.22823023796081543 seconds\n",
      "TIMESTEP 583009 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23481535911560059 seconds\n",
      "TIMESTEP 583010 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23348689079284668 seconds\n",
      "TIMESTEP 583011 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23311948776245117 seconds\n",
      "TIMESTEP 583012 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23422574996948242 seconds\n",
      "TIMESTEP 583013 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23400592803955078 seconds\n",
      "TIMESTEP 583014 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.5063211917877197 seconds\n",
      "TIMESTEP 583015 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD -1.0 / Q_MAX  0 / Loss  0\n",
      "loop took 0.22901153564453125 seconds\n",
      "TIMESTEP 583016 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.21849489212036133 seconds\n",
      "TIMESTEP 583017 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23192191123962402 seconds\n",
      "TIMESTEP 583018 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.26688337326049805 seconds\n",
      "TIMESTEP 583019 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2648043632507324 seconds\n",
      "TIMESTEP 583020 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.26642370223999023 seconds\n",
      "TIMESTEP 583021 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.5211455821990967 seconds\n",
      "TIMESTEP 583022 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD -1.0 / Q_MAX  0 / Loss  0\n",
      "loop took 0.22843217849731445 seconds\n",
      "TIMESTEP 583023 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.21825861930847168 seconds\n",
      "TIMESTEP 583024 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23548221588134766 seconds\n",
      "TIMESTEP 583025 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2491292953491211 seconds\n",
      "TIMESTEP 583026 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23543858528137207 seconds\n",
      "TIMESTEP 583027 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24911808967590332 seconds\n",
      "TIMESTEP 583028 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2674539089202881 seconds\n",
      "TIMESTEP 583029 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23303604125976562 seconds\n",
      "TIMESTEP 583030 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24974822998046875 seconds\n",
      "TIMESTEP 583031 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.26723241806030273 seconds\n",
      "TIMESTEP 583032 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24850893020629883 seconds\n",
      "TIMESTEP 583033 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.26618313789367676 seconds\n",
      "TIMESTEP 583034 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24733209609985352 seconds\n",
      "TIMESTEP 583035 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.281491756439209 seconds\n",
      "TIMESTEP 583036 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.28194165229797363 seconds\n",
      "TIMESTEP 583037 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2825348377227783 seconds\n",
      "TIMESTEP 583038 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.3130931854248047 seconds\n",
      "TIMESTEP 583039 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24845075607299805 seconds\n",
      "TIMESTEP 583040 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.5217986106872559 seconds\n",
      "TIMESTEP 583041 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD -0.36666666666666664 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24290943145751953 seconds\n",
      "TIMESTEP 583042 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.21862077713012695 seconds\n",
      "TIMESTEP 583043 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23530817031860352 seconds\n",
      "TIMESTEP 583044 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2501821517944336 seconds\n",
      "TIMESTEP 583045 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2340695858001709 seconds\n",
      "TIMESTEP 583046 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2489757537841797 seconds\n",
      "TIMESTEP 583047 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23621535301208496 seconds\n",
      "TIMESTEP 583048 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2480463981628418 seconds\n",
      "TIMESTEP 583049 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24946188926696777 seconds\n",
      "TIMESTEP 583050 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.250943660736084 seconds\n",
      "TIMESTEP 583051 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23302531242370605 seconds\n",
      "TIMESTEP 583052 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2519354820251465 seconds\n",
      "TIMESTEP 583053 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23378610610961914 seconds\n",
      "TIMESTEP 583054 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23538708686828613 seconds\n",
      "TIMESTEP 583055 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.265728235244751 seconds\n",
      "TIMESTEP 583056 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2475416660308838 seconds\n",
      "TIMESTEP 583057 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.5207128524780273 seconds\n",
      "TIMESTEP 583058 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD -0.4230769230769231 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24479198455810547 seconds\n",
      "TIMESTEP 583059 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24936175346374512 seconds\n",
      "TIMESTEP 583060 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.25190234184265137 seconds\n",
      "TIMESTEP 583061 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23510408401489258 seconds\n",
      "TIMESTEP 583062 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop took 0.25002074241638184 seconds\n",
      "TIMESTEP 583063 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.27872323989868164 seconds\n",
      "TIMESTEP 583064 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.5241258144378662 seconds\n",
      "TIMESTEP 583065 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD -1.0 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2272498607635498 seconds\n",
      "TIMESTEP 583066 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2668271064758301 seconds\n",
      "TIMESTEP 583067 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2511861324310303 seconds\n",
      "TIMESTEP 583068 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.26635241508483887 seconds\n",
      "TIMESTEP 583069 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2465507984161377 seconds\n",
      "TIMESTEP 583070 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.280407190322876 seconds\n",
      "TIMESTEP 583071 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.5279467105865479 seconds\n",
      "TIMESTEP 583072 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD -1.0 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23838257789611816 seconds\n",
      "TIMESTEP 583073 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24938035011291504 seconds\n",
      "TIMESTEP 583074 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24940967559814453 seconds\n",
      "TIMESTEP 583075 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2667977809906006 seconds\n",
      "TIMESTEP 583076 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2503671646118164 seconds\n",
      "TIMESTEP 583077 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.27166295051574707 seconds\n",
      "TIMESTEP 583078 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.5161166191101074 seconds\n",
      "TIMESTEP 583079 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD -1.0 / Q_MAX  0 / Loss  0\n",
      "loop took 0.22708821296691895 seconds\n",
      "TIMESTEP 583080 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23380541801452637 seconds\n",
      "TIMESTEP 583081 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23523831367492676 seconds\n",
      "TIMESTEP 583082 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2661466598510742 seconds\n",
      "TIMESTEP 583083 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.21902036666870117 seconds\n",
      "TIMESTEP 583084 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2326221466064453 seconds\n",
      "TIMESTEP 583085 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23498058319091797 seconds\n",
      "TIMESTEP 583086 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.25018811225891113 seconds\n",
      "TIMESTEP 583087 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.25014376640319824 seconds\n",
      "TIMESTEP 583088 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23363876342773438 seconds\n",
      "TIMESTEP 583089 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24946260452270508 seconds\n",
      "TIMESTEP 583090 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.4932377338409424 seconds\n",
      "TIMESTEP 583091 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD -0.6111111111111112 / Q_MAX  0 / Loss  0\n",
      "loop took 0.22478079795837402 seconds\n",
      "TIMESTEP 583092 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23507142066955566 seconds\n",
      "TIMESTEP 583093 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23410391807556152 seconds\n",
      "TIMESTEP 583094 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.25080323219299316 seconds\n",
      "TIMESTEP 583095 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24180841445922852 seconds\n",
      "TIMESTEP 583096 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.257596492767334 seconds\n",
      "TIMESTEP 583097 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.5198953151702881 seconds\n",
      "TIMESTEP 583098 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD -1.0 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2463512420654297 seconds\n",
      "TIMESTEP 583099 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23355460166931152 seconds\n",
      "TIMESTEP 583100 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23343944549560547 seconds\n",
      "TIMESTEP 583101 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.40584850311279297 seconds\n",
      "TIMESTEP 583102 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.21940350532531738 seconds\n",
      "TIMESTEP 583103 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24952006340026855 seconds\n",
      "TIMESTEP 583104 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.536287784576416 seconds\n",
      "TIMESTEP 583105 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD -1.0 / Q_MAX  0 / Loss  0\n",
      "loop took 0.22969961166381836 seconds\n",
      "TIMESTEP 583106 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2180180549621582 seconds\n",
      "TIMESTEP 583107 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23614192008972168 seconds\n",
      "TIMESTEP 583108 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24934792518615723 seconds\n",
      "TIMESTEP 583109 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23340535163879395 seconds\n",
      "TIMESTEP 583110 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.25194644927978516 seconds\n",
      "TIMESTEP 583111 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.5057036876678467 seconds\n",
      "TIMESTEP 583112 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD -1.0 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2279195785522461 seconds\n",
      "TIMESTEP 583113 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.249497652053833 seconds\n",
      "TIMESTEP 583114 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2350454330444336 seconds\n",
      "TIMESTEP 583115 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24868988990783691 seconds\n",
      "TIMESTEP 583116 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.26608991622924805 seconds\n",
      "TIMESTEP 583117 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2493739128112793 seconds\n",
      "TIMESTEP 583118 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.5073146820068359 seconds\n",
      "TIMESTEP 583119 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD -1.0 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24433088302612305 seconds\n",
      "TIMESTEP 583120 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23313617706298828 seconds\n",
      "TIMESTEP 583121 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2359321117401123 seconds\n",
      "TIMESTEP 583122 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.25063300132751465 seconds\n",
      "TIMESTEP 583123 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.250577449798584 seconds\n",
      "TIMESTEP 583124 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23353028297424316 seconds\n",
      "TIMESTEP 583125 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop took 0.48923635482788086 seconds\n",
      "TIMESTEP 583126 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD -1.0 / Q_MAX  0 / Loss  0\n",
      "loop took 0.22818660736083984 seconds\n",
      "TIMESTEP 583127 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2341005802154541 seconds\n",
      "TIMESTEP 583128 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2324364185333252 seconds\n",
      "TIMESTEP 583129 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.250032901763916 seconds\n",
      "TIMESTEP 583130 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.25005197525024414 seconds\n",
      "TIMESTEP 583131 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24944114685058594 seconds\n",
      "TIMESTEP 583132 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.5062158107757568 seconds\n",
      "TIMESTEP 583133 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD -1.0 / Q_MAX  0 / Loss  0\n",
      "loop took 0.21215319633483887 seconds\n",
      "TIMESTEP 583134 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2348647117614746 seconds\n",
      "TIMESTEP 583135 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.21997833251953125 seconds\n",
      "TIMESTEP 583136 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.26488471031188965 seconds\n",
      "TIMESTEP 583137 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23258042335510254 seconds\n",
      "TIMESTEP 583138 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.25124263763427734 seconds\n",
      "TIMESTEP 583139 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2356250286102295 seconds\n",
      "TIMESTEP 583140 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.25167036056518555 seconds\n",
      "TIMESTEP 583141 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24464917182922363 seconds\n",
      "TIMESTEP 583142 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.26792383193969727 seconds\n",
      "TIMESTEP 583143 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2703249454498291 seconds\n",
      "TIMESTEP 583144 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.5018301010131836 seconds\n",
      "TIMESTEP 583145 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD -0.5789473684210527 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24554848670959473 seconds\n",
      "TIMESTEP 583146 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24839425086975098 seconds\n",
      "TIMESTEP 583147 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24745416641235352 seconds\n",
      "TIMESTEP 583148 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2723557949066162 seconds\n",
      "TIMESTEP 583149 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24309945106506348 seconds\n",
      "TIMESTEP 583150 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2650628089904785 seconds\n",
      "TIMESTEP 583151 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.523231029510498 seconds\n",
      "TIMESTEP 583152 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD -1.0 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2300260066986084 seconds\n",
      "TIMESTEP 583153 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23289823532104492 seconds\n",
      "TIMESTEP 583154 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.21817493438720703 seconds\n",
      "TIMESTEP 583155 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.25003981590270996 seconds\n",
      "TIMESTEP 583156 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23424482345581055 seconds\n",
      "TIMESTEP 583157 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2503175735473633 seconds\n",
      "TIMESTEP 583158 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.5043048858642578 seconds\n",
      "TIMESTEP 583159 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD -1.0 / Q_MAX  0 / Loss  0\n",
      "loop took 0.21312689781188965 seconds\n",
      "TIMESTEP 583160 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.21826791763305664 seconds\n",
      "TIMESTEP 583161 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2327098846435547 seconds\n",
      "TIMESTEP 583162 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.251126766204834 seconds\n",
      "TIMESTEP 583163 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23455023765563965 seconds\n",
      "TIMESTEP 583164 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.26549410820007324 seconds\n",
      "TIMESTEP 583165 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.5055584907531738 seconds\n",
      "TIMESTEP 583166 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD -1.0 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24466800689697266 seconds\n",
      "TIMESTEP 583167 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2324533462524414 seconds\n",
      "TIMESTEP 583168 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2351391315460205 seconds\n",
      "TIMESTEP 583169 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.26624631881713867 seconds\n",
      "TIMESTEP 583170 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23208093643188477 seconds\n",
      "TIMESTEP 583171 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.25082969665527344 seconds\n",
      "TIMESTEP 583172 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.5069336891174316 seconds\n",
      "TIMESTEP 583173 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD -1.0 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24326252937316895 seconds\n",
      "TIMESTEP 583174 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2514808177947998 seconds\n",
      "TIMESTEP 583175 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2500913143157959 seconds\n",
      "TIMESTEP 583176 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.26627063751220703 seconds\n",
      "TIMESTEP 583177 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.20154762268066406 seconds\n",
      "TIMESTEP 583178 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2740919589996338 seconds\n",
      "TIMESTEP 583179 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.49707555770874023 seconds\n",
      "TIMESTEP 583180 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD -1.0 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2282848358154297 seconds\n",
      "TIMESTEP 583181 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.21716737747192383 seconds\n",
      "TIMESTEP 583182 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.21930456161499023 seconds\n",
      "TIMESTEP 583183 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2501332759857178 seconds\n",
      "TIMESTEP 583184 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24965715408325195 seconds\n",
      "TIMESTEP 583185 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2504551410675049 seconds\n",
      "TIMESTEP 583186 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.505056619644165 seconds\n",
      "TIMESTEP 583187 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD -1.0 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24482107162475586 seconds\n",
      "TIMESTEP 583188 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop took 0.23276567459106445 seconds\n",
      "TIMESTEP 583189 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23379230499267578 seconds\n",
      "TIMESTEP 583190 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2510371208190918 seconds\n",
      "TIMESTEP 583191 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24947452545166016 seconds\n",
      "TIMESTEP 583192 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.26323461532592773 seconds\n",
      "TIMESTEP 583193 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.5306010246276855 seconds\n",
      "TIMESTEP 583194 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD -1.0 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23646140098571777 seconds\n",
      "TIMESTEP 583195 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23197460174560547 seconds\n",
      "TIMESTEP 583196 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2345280647277832 seconds\n",
      "TIMESTEP 583197 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2697746753692627 seconds\n",
      "TIMESTEP 583198 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2615501880645752 seconds\n",
      "TIMESTEP 583199 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24946331977844238 seconds\n",
      "TIMESTEP 583200 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.4940969944000244 seconds\n",
      "TIMESTEP 583201 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD -1.0 / Q_MAX  0 / Loss  0\n",
      "loop took 0.34969520568847656 seconds\n",
      "TIMESTEP 583202 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23317193984985352 seconds\n",
      "TIMESTEP 583203 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23507165908813477 seconds\n",
      "TIMESTEP 583204 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23301076889038086 seconds\n",
      "TIMESTEP 583205 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.25063085556030273 seconds\n",
      "TIMESTEP 583206 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2695498466491699 seconds\n",
      "TIMESTEP 583207 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.4883294105529785 seconds\n",
      "TIMESTEP 583208 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD -0.9166666666666666 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24280858039855957 seconds\n",
      "TIMESTEP 583209 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23444557189941406 seconds\n",
      "TIMESTEP 583210 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2334439754486084 seconds\n",
      "TIMESTEP 583211 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24952173233032227 seconds\n",
      "TIMESTEP 583212 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2695045471191406 seconds\n",
      "TIMESTEP 583213 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.27767157554626465 seconds\n",
      "TIMESTEP 583214 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.5217576026916504 seconds\n",
      "TIMESTEP 583215 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD -1.0 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24338126182556152 seconds\n",
      "TIMESTEP 583216 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23445868492126465 seconds\n",
      "TIMESTEP 583217 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23327302932739258 seconds\n",
      "TIMESTEP 583218 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2495734691619873 seconds\n",
      "TIMESTEP 583219 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2682185173034668 seconds\n",
      "TIMESTEP 583220 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.26409173011779785 seconds\n",
      "TIMESTEP 583221 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.5205903053283691 seconds\n",
      "TIMESTEP 583222 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD -1.0 / Q_MAX  0 / Loss  0\n",
      "loop took 0.21356773376464844 seconds\n",
      "TIMESTEP 583223 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.21958446502685547 seconds\n",
      "TIMESTEP 583224 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.22058796882629395 seconds\n",
      "TIMESTEP 583225 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24765706062316895 seconds\n",
      "TIMESTEP 583226 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2326202392578125 seconds\n",
      "TIMESTEP 583227 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.22058558464050293 seconds\n",
      "TIMESTEP 583228 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.21859145164489746 seconds\n",
      "TIMESTEP 583229 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.5166721343994141 seconds\n",
      "TIMESTEP 583230 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD -1.0 / Q_MAX  0 / Loss  0\n",
      "loop took 0.21657919883728027 seconds\n",
      "TIMESTEP 583231 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.21958208084106445 seconds\n",
      "TIMESTEP 583232 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.21757936477661133 seconds\n",
      "TIMESTEP 583233 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2964012622833252 seconds\n",
      "TIMESTEP 583234 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.29793787002563477 seconds\n",
      "TIMESTEP 583235 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.28029775619506836 seconds\n",
      "TIMESTEP 583236 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2650132179260254 seconds\n",
      "TIMESTEP 583237 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.26546764373779297 seconds\n",
      "TIMESTEP 583238 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2494363784790039 seconds\n",
      "TIMESTEP 583239 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24976897239685059 seconds\n",
      "TIMESTEP 583240 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.5220487117767334 seconds\n",
      "TIMESTEP 583241 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD -0.6111111111111112 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24435687065124512 seconds\n",
      "TIMESTEP 583242 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23334860801696777 seconds\n",
      "TIMESTEP 583243 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2348785400390625 seconds\n",
      "TIMESTEP 583244 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.264664888381958 seconds\n",
      "TIMESTEP 583245 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24927330017089844 seconds\n",
      "TIMESTEP 583246 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2512967586517334 seconds\n",
      "TIMESTEP 583247 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.5051026344299316 seconds\n",
      "TIMESTEP 583248 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD -1.0 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2281477451324463 seconds\n",
      "TIMESTEP 583249 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2205667495727539 seconds\n",
      "TIMESTEP 583250 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop took 0.2185511589050293 seconds\n",
      "TIMESTEP 583251 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2501978874206543 seconds\n",
      "TIMESTEP 583252 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23140740394592285 seconds\n",
      "TIMESTEP 583253 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2542755603790283 seconds\n",
      "TIMESTEP 583254 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2327263355255127 seconds\n",
      "TIMESTEP 583255 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2646772861480713 seconds\n",
      "TIMESTEP 583256 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.26485466957092285 seconds\n",
      "TIMESTEP 583257 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2512850761413574 seconds\n",
      "TIMESTEP 583258 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24902892112731934 seconds\n",
      "TIMESTEP 583259 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2654874324798584 seconds\n",
      "TIMESTEP 583260 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.26317882537841797 seconds\n",
      "TIMESTEP 583261 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2643566131591797 seconds\n",
      "TIMESTEP 583262 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23507404327392578 seconds\n",
      "TIMESTEP 583263 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.5103480815887451 seconds\n",
      "TIMESTEP 583264 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD -0.44 / Q_MAX  0 / Loss  0\n",
      "loop took 0.255462646484375 seconds\n",
      "TIMESTEP 583265 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.25406765937805176 seconds\n",
      "TIMESTEP 583266 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.26134252548217773 seconds\n",
      "TIMESTEP 583267 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2656590938568115 seconds\n",
      "TIMESTEP 583268 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.21672916412353516 seconds\n",
      "TIMESTEP 583269 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2826991081237793 seconds\n",
      "TIMESTEP 583270 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.5445945262908936 seconds\n",
      "TIMESTEP 583271 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD -1.0 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2364978790283203 seconds\n",
      "TIMESTEP 583272 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23456573486328125 seconds\n",
      "TIMESTEP 583273 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24884557723999023 seconds\n",
      "TIMESTEP 583274 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2714993953704834 seconds\n",
      "TIMESTEP 583275 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24373626708984375 seconds\n",
      "TIMESTEP 583276 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2816910743713379 seconds\n",
      "TIMESTEP 583277 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.5222690105438232 seconds\n",
      "TIMESTEP 583278 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD -1.0 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24347305297851562 seconds\n",
      "TIMESTEP 583279 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23441290855407715 seconds\n",
      "TIMESTEP 583280 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23305201530456543 seconds\n",
      "TIMESTEP 583281 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2666313648223877 seconds\n",
      "TIMESTEP 583282 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.25046586990356445 seconds\n",
      "TIMESTEP 583283 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23285222053527832 seconds\n",
      "TIMESTEP 583284 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.5238571166992188 seconds\n",
      "TIMESTEP 583285 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD -1.0 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24346923828125 seconds\n",
      "TIMESTEP 583286 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2315819263458252 seconds\n",
      "TIMESTEP 583287 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23657655715942383 seconds\n",
      "TIMESTEP 583288 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2630655765533447 seconds\n",
      "TIMESTEP 583289 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2670576572418213 seconds\n",
      "TIMESTEP 583290 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.263927698135376 seconds\n",
      "TIMESTEP 583291 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.5226945877075195 seconds\n",
      "TIMESTEP 583292 / STATE observe / EPSILON 0.0001 / ACTION 1 / REWARD -1.0 / Q_MAX  0 / Loss  0\n",
      "loop took 0.226837158203125 seconds\n",
      "TIMESTEP 583293 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23490262031555176 seconds\n",
      "TIMESTEP 583294 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.20524883270263672 seconds\n",
      "TIMESTEP 583295 / STATE observe / EPSILON 0.0001 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-bd1e780b09e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#searchforplay\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplayGame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobserve\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-2255e85230c6>\u001b[0m in \u001b[0;36mplayGame\u001b[1;34m(observe)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mgame_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGame_sate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdino\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuildmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtrainNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgame_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mobserve\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobserve\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-4ddf7cf1ad9e>\u001b[0m in \u001b[0;36mtrainNetwork\u001b[1;34m(model, game_state, observe)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;31m#run the selected action and observed next state and reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[0mx_t1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgame_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loop took {} seconds'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlast_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mlast_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-b5615727d5f2>\u001b[0m in \u001b[0;36mget_state\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrab_screen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_crashed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#searchforplay\n",
    "playGame(observe=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def show_plots(realtime = True,t=0):\n",
    "    fig, axs = plt.subplots(ncols=2,nrows =2)\n",
    "    loss_df = pd.read_csv(\"./objects/loss_df.csv\")\n",
    "    scores_df = pd.read_csv(\"./objects/scores_df.csv\")\n",
    "    actions_df = pd.read_csv(\"./objects/actions_df.csv\")\n",
    "    q_max_df = pd.read_csv(\"./objects/q_values.csv\")\n",
    "    loss_df['loss'] = loss_df['loss'].astype('float') \n",
    "    loss_df.plot(use_index=True,ax=axs[0,0])\n",
    "    scores_df.plot(ax=axs[0,1])\n",
    "    sns.distplot(actions_df.tail(20000),ax=axs[1,0])\n",
    "#     q_max_df.plot(ax = axs[1,1])\n",
    "    imgg = fig.canvas.draw()\n",
    "    graph_img = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
    "    graph_img = graph_img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "#     disp = show_img(graphs=True)\n",
    "#     disp.__next__()\n",
    "    cv2.imwrite(\"logs/progress/pg\"+str(t)+\".png\",graph_img) if realtime else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_plots(realtime=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores_df.tail(1000).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
