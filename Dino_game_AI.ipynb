{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPEN_CV AND PIL IMPORTS\n",
    "import numpy as np\n",
    "from PIL import ImageGrab\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LIBRARY IMPORTS\n",
    "%matplotlib inline \n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (15, 9)\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SELENIUM IMPORTS\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#KERAS IMPORTS\n",
    "%matplotlib inline \n",
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD , Adam\n",
    "from keras.callbacks import TensorBoard\n",
    "from collections import deque\n",
    "import random\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PATH VARIABLES\n",
    "game_url = \"game/dino.html\"\n",
    "chrome_driver_path = \"./chromedriver.exe\"\n",
    "loss_file_path = \"./objects/loss_df.csv\"\n",
    "actions_file_path = \"./objects/actions_df.csv\"\n",
    "scores_file_path = \"./objects/scores_df.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GAME MODULE\n",
    "'''\n",
    "* Game class: Selenium interfacing between the python and browser\n",
    "* __init__():  Launch the broswer window using the attributes in chrome_options\n",
    "* crashed() : return true if the agent as crashed on an obstacles. Gets javascript variable from game decribing the state\n",
    "* playing(): true if game in progress, false is crashed or paused\n",
    "* restart() : sends a signal to browser-javascript to restart the game\n",
    "* press_up(): sends a single to press up get to the browser\n",
    "* get_score(): gets current game score from javascript variables.\n",
    "* pause(): pause the game\n",
    "* resume(): resume a paused game if not crashed\n",
    "* end(): close the browser and end the game\n",
    "'''\n",
    "class Game:\n",
    "    def __init__(self,custom_config=True):\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"disable-infobars\")\n",
    "        self._driver = webdriver.Chrome(executable_path = chrome_driver_path,chrome_options=chrome_options)\n",
    "        self._driver.set_window_position(x=-10,y=0)\n",
    "        self._driver.set_window_size(200, 300)\n",
    "        self._driver.get(os.path.abspath(game_url))\n",
    "        #modifying game before trainNetworkining\n",
    "        if custom_config:\n",
    "            self._driver.execute_script(\"Runner.config.ACCELERATION=0\")\n",
    "    def crashed(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.crashed\")\n",
    "    def playing(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.playing\")\n",
    "    def restart(self):\n",
    "        self._driver.execute_script(\"Runner.instance_.restart()\")\n",
    "        \n",
    "        time.sleep(0.25)# no actions are possible \n",
    "                        # for 0.25 sec after game starts, \n",
    "                        # skip learning at this time and make the model wait\n",
    "    def press_up(self):\n",
    "        self._driver.find_element_by_tag_name(\"body\").send_keys(Keys.ARROW_UP)\n",
    "    def get_score(self):\n",
    "        score_array = self._driver.execute_script(\"return Runner.instance_.distanceMeter.digits\")\n",
    "        score = ''.join(score_array) # the javascript object is of type array with score in the formate[1,0,0] which is 100.\n",
    "        return int(score)\n",
    "    def pause(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.stop()\")\n",
    "    def resume(self):\n",
    "        return self._driver.execute_script(\"return Runner.instance_.play()\")\n",
    "    def end(self):\n",
    "        self._driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AGENT MODULE\n",
    "class Agent_Dino:\n",
    "    def __init__(self,game): #takes game as input for taking actions\n",
    "        self._game = game; \n",
    "        self.jump(); #to start the game, we need to jump once\n",
    "        time.sleep(.5) # no action can be performed for the first time when game starts\n",
    "    def is_running(self):\n",
    "        return self._game.playing()\n",
    "    def is_crashed(self):\n",
    "        return self._game.crashed()\n",
    "    def jump(self):\n",
    "        self._game.press_up()\n",
    "    def duck(self):\n",
    "        self._game.press_down()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GAME STATE MODULE\n",
    "class Game_state:\n",
    "    def __init__(self,agent,game):\n",
    "        self._agent = agent\n",
    "        self._game = game\n",
    "        self._display = display_images() #display the processed image on screen using openCV, implemented using python coroutine \n",
    "        self._display.__next__() # initiliaze the display coroutine \n",
    "    def get_state(self,actions):\n",
    "        actions_df.loc[len(actions_df)] = actions[1] # storing actions in a dataframe\n",
    "        score = self._game.get_score() \n",
    "        reward = 0.1*score/10 # dynamic reward calculation\n",
    "        is_over = False #game over\n",
    "        if actions[1] == 1:\n",
    "            self._agent.jump()\n",
    "            reward = 0.1*score/11\n",
    "        image = screen_capture() \n",
    "        self._display.send(image) #display the image on screen\n",
    "\n",
    "        if self._agent.is_crashed():\n",
    "            scores_df.loc[len(loss_df)] = score # log the score when game is over\n",
    "            self._game.restart()\n",
    "            reward = -11/score\n",
    "            is_over = True\n",
    "        return image, reward, is_over #return the Experience tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(obj, name ):\n",
    "    with open('objects/'+ name + '.pkl', 'wb') as f: #dump files into objects folder\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load(name):\n",
    "    with open('objects/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "def screen_capture(_driver = None):\n",
    "    screen =  np.array(ImageGrab.grab(bbox=(40,180,440,400))) #bbox = region of interset on the entire screen\n",
    "    image = image_processing(screen)#processing image as required\n",
    "    return image\n",
    "\n",
    "def image_processing(image):\n",
    "    #game is already in grey scale canvas, canny to get only edges and reduce unwanted objects(clouds)\n",
    "    image = cv2.resize(image, (0,0), fx = 0.15, fy = 0.10) # resale image dimensions\n",
    "    image = image[2:38,10:50] #img[y:y+h, x:x+w] #crop out the dino agent from the frame\n",
    "    image = cv2.Canny(image, threshold1 = 100, threshold2 = 200) #apply the canny edge detection\n",
    "    return  image\n",
    "def display_images(graphs = False):\n",
    "    \"\"\"\n",
    "    Show images in new window\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        screen = (yield)\n",
    "        window_title = \"logs\" if graphs else \"game_play\"\n",
    "        cv2.namedWindow(window_title, cv2.WINDOW_NORMAL)        \n",
    "        imS = cv2.resize(screen, (800, 400)) \n",
    "        cv2.imshow(window_title, screen)\n",
    "        if (cv2.waitKey(1) & 0xFF == ord('q')):\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize log structures from file if exists else create new\n",
    "loss_df = pd.read_csv(loss_file_path) if os.path.isfile(loss_file_path) else pd.DataFrame(columns =['loss'])\n",
    "scores_df = pd.read_csv(scores_file_path) if os.path.isfile(loss_file_path) else pd.DataFrame(columns = ['scores'])\n",
    "actions_df = pd.read_csv(actions_file_path) if os.path.isfile(actions_file_path) else pd.DataFrame(columns = ['actions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training variables saved as checkpoints to filesystem to resume training from the same step\n",
    "def init_cache():\n",
    "    \"\"\"initial variable caching, done only once\"\"\"\n",
    "    save(INITIAL_EPSILON,\"epsilon\")\n",
    "    t = 0\n",
    "    save(t,\"time\")\n",
    "    D = deque()\n",
    "    save(D,\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#game parameters\n",
    "ACTIONS = 2 # possible actions: jump, do nothing\n",
    "GAMMA = 0.99 # decay rate of past observations original 0.99\n",
    "OBSERVATION = 50000. # timesteps to observe before training\n",
    "EXPLORE = 100000  # frames over which to anneal epsilon\n",
    "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
    "INITIAL_EPSILON = 0.1 # starting value of epsilon\n",
    "REPLAY_MEMORY = 50000 # number of previous transitions to remember\n",
    "BATCH = 32 # size of minibatch\n",
    "FRAME_PER_ACTION = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "image_rows , image_columns = 40,20\n",
    "image_channels = 4 #We stack 4 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL BUILDING\n",
    "def buildmodel():\n",
    "    print(\"Now we build the model\")\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (8, 8), strides=(4, 4), padding='same',input_shape=(image_columns,image_rows,image_channels)))  #20*40*4\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (4, 4), strides=(2, 2), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3), strides=(1, 1), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(ACTIONS))\n",
    "    adam = Adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='mse',optimizer=adam)\n",
    "    print(\"We finish building the model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we build the model\n",
      "WARNING:tensorflow:From C:\\Users\\Acer\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "We finish building the model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 5, 10, 32)         8224      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5, 10, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 5, 64)          32832     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 3, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 5, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 3, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 960)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               492032    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 571,042\n",
      "Trainable params: 571,042\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#MODEL ARCHITECTURE\n",
    "buildmodel().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAIN TRAINING MODULE\n",
    "''' \n",
    "main training module\n",
    "Parameters:\n",
    "* model => Keras Model to be trained\n",
    "* game_state => Game State module with access to game environment and dino\n",
    "* observe => flag to indicate wherther the model is to be trained(weight updates), else just play\n",
    "'''\n",
    "def trainNetwork(model,game_state,observe=False):\n",
    "    last_time = time.time()\n",
    "    # store the previous observations in replay memory\n",
    "    D = load(\"D\") #load from file system\n",
    "    # get the first state by doing nothing\n",
    "    do_nothing = np.zeros(ACTIONS)\n",
    "    do_nothing[0] =1 #0 => do nothing,\n",
    "                     #1=> jump\n",
    "    \n",
    "    x_t, r_0, terminal = game_state.get_state(do_nothing) # get next step after performing the action\n",
    "    \n",
    "\n",
    "    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2) # stack 4 images to create placeholder input\n",
    "    \n",
    "\n",
    "    \n",
    "    s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])  #1*20*40*4\n",
    "    \n",
    "    initial_state = s_t \n",
    "\n",
    "    if observe :\n",
    "        OBSERVE = 999999999    #We keep observe, never train\n",
    "        epsilon1 = FINAL_EPSILON\n",
    "        print (\"Now we load weight\")\n",
    "        model.load_weights(\"model_final.h5\")\n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "        print (\"Weight load successfully\")    \n",
    "    else:                       #We go to training mode\n",
    "        OBSERVE = OBSERVATION\n",
    "        epsilon = load(\"epsilon\") \n",
    "        model.load_weights(\"model_final.h5\")\n",
    "        adam = Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "\n",
    "    t = load(\"time\") # resume from the previous time step stored in file system\n",
    "    while (True): #endless running\n",
    "        \n",
    "        loss = 0\n",
    "        Q_sa = 0\n",
    "        action_index = 0\n",
    "        r_t = 0 #reward at 4\n",
    "        a_t = np.zeros([ACTIONS]) # action at t\n",
    "        \n",
    "        #choose an action epsilon greedy\n",
    "        if t % FRAME_PER_ACTION == 0: #parameter to skip frames for actions\n",
    "            if  random.random() <= epsilon: #randomly explore an action\n",
    "                print(\"----------Random Action----------\")\n",
    "                action_index = random.randrange(ACTIONS)\n",
    "                a_t[0] = 1\n",
    "            else: # predict the output\n",
    "                q = model.predict(s_t)       #input a stack of 4 images, get the prediction\n",
    "                max_Q = np.argmax(q)         # chosing index with maximum q value\n",
    "                action_index = max_Q \n",
    "                a_t[action_index] = 1        # o=> do nothing, 1=> jump\n",
    "                \n",
    "        #We reduced the epsilon (exploration parameter) gradually\n",
    "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
    "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE \n",
    "\n",
    "        #run the selected action and observed next state and reward\n",
    "        x_t1, r_t, terminal = game_state.get_state(a_t)\n",
    "        print('loop took {} seconds'.format(time.time()-last_time)) # helpful for measuring frame rate\n",
    "        last_time = time.time()\n",
    "        x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1], 1) #1x20x40x1\n",
    "        s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3) # append the new image to input stack and remove the first one\n",
    "        \n",
    "        \n",
    "        # store the transition in D\n",
    "        D.append((s_t, action_index, r_t, s_t1, terminal))\n",
    "        if len(D) > REPLAY_MEMORY:\n",
    "            D.popleft()\n",
    "\n",
    "        #only train if done observing\n",
    "        if t > OBSERVE: \n",
    "            \n",
    "            #sample a minibatch to train on\n",
    "            minibatch = random.sample(D, BATCH)\n",
    "            inputs = np.zeros((BATCH, s_t.shape[1], s_t.shape[2], s_t.shape[3]))   #32, 20, 40, 4\n",
    "            targets = np.zeros((inputs.shape[0], ACTIONS))                         #32, 2\n",
    "\n",
    "            #Now we do the experience replay\n",
    "            for i in range(0, len(minibatch)):\n",
    "                state_t = minibatch[i][0]    # 4D stack of images\n",
    "                action_t = minibatch[i][1]   #This is action index\n",
    "                reward_t = minibatch[i][2]   #reward at state_t due to action_t\n",
    "                state_t1 = minibatch[i][3]   #next state\n",
    "                terminal = minibatch[i][4]   #wheather the agent died or survided due the action\n",
    "                \n",
    "\n",
    "                inputs[i:i + 1] = state_t    \n",
    "\n",
    "                targets[i] = model.predict(state_t)  # predicted q values\n",
    "                Q_sa = model.predict(state_t1)      #predict q values for next step\n",
    "                \n",
    "                if terminal:\n",
    "                    targets[i, action_t] = reward_t # if terminated, only equals reward\n",
    "                else:\n",
    "                    targets[i, action_t] = reward_t + GAMMA * np.max(Q_sa)\n",
    "\n",
    "            loss += model.train_on_batch(inputs, targets)\n",
    "            loss_df.loc[len(loss_df)] = loss\n",
    "        else:\n",
    "            # artificial time delay as training done with this delay\n",
    "            time.sleep(0.12)\n",
    "        s_t = initial_state if terminal else s_t1 #reset game to initial frame if terminate\n",
    "        t = t + 1\n",
    "        \n",
    "        # save progress every 1000 iterations\n",
    "        if t % 1000 == 0:\n",
    "            print(\"Now we save model\")\n",
    "            \n",
    "            model.save_weights(\"model_final.h5\", overwrite=True)\n",
    "            save(D,\"D\") #saving episodes\n",
    "            save(t,\"time\") #caching time steps\n",
    "            save(epsilon,\"epsilon\") #cache epsilon to avoid repeated randomness in actions\n",
    "            loss_df.to_csv(\"./objects/loss_df.csv\",index=False)\n",
    "            scores_df.to_csv(\"./objects/scores_df.csv\",index=False)\n",
    "            actions_df.to_csv(\"./objects/actions_df.csv\",index=False)\n",
    "            with open(\"model.json\", \"w\") as outfile:\n",
    "                json.dump(model.to_json(), outfile)\n",
    "\n",
    "        # print info\n",
    "        state = \"\"\n",
    "        if t <= OBSERVE:\n",
    "            state = \"observe\"\n",
    "        elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
    "            state = \"explore\"\n",
    "        else:\n",
    "            state = \"train\"\n",
    "\n",
    "        print(\"TIMESTEP\", t, \"/ STATE\", state,             \"/ EPSILON\", epsilon, \"/ ACTION\", action_index, \"/ REWARD\", r_t,             \"/ Q_MAX \" , np.max(Q_sa), \"/ Loss \", loss)\n",
    "\n",
    "    print(\"Episode finished!\")\n",
    "    print(\"************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def playGame(observe=False):\n",
    "    game = Game()\n",
    "    dino = Agent_Dino(game)\n",
    "    game_state = Game_state(dino,game)\n",
    "    model = buildmodel()\n",
    "    try:\n",
    "        trainNetwork(model,game_state,observe=observe)\n",
    "    except StopIteration:\n",
    "        game.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: DeprecationWarning: use options instead of chrome_options\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we build the model\n",
      "We finish building the model\n",
      "----------Random Action----------\n",
      "loop took 0.8432583808898926 seconds\n",
      "TIMESTEP 1 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.07 / Q_MAX  0 / Loss  0\n",
      "loop took 0.5654876232147217 seconds\n",
      "TIMESTEP 2 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.27227163314819336 seconds\n",
      "TIMESTEP 3 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1272727272727273 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24982857704162598 seconds\n",
      "TIMESTEP 4 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.14545454545454548 / Q_MAX  0 / Loss  0\n",
      "loop took 0.25930237770080566 seconds\n",
      "TIMESTEP 5 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.17272727272727273 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2822229862213135 seconds\n",
      "TIMESTEP 6 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.19090909090909092 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2812471389770508 seconds\n",
      "TIMESTEP 7 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.2090909090909091 / Q_MAX  0 / Loss  0\n",
      "loop took 0.26628875732421875 seconds\n",
      "TIMESTEP 8 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.23636363636363636 / Q_MAX  0 / Loss  0\n",
      "loop took 0.3121609687805176 seconds\n",
      "TIMESTEP 9 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.26363636363636367 / Q_MAX  0 / Loss  0\n",
      "loop took 0.5027198791503906 seconds\n",
      "TIMESTEP 10 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -0.3548387096774194 / Q_MAX  0 / Loss  0\n",
      "loop took 0.27897024154663086 seconds\n",
      "TIMESTEP 11 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.03636363636363637 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "loop took 0.22639679908752441 seconds\n",
      "TIMESTEP 12 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.06000000000000001 / Q_MAX  0 / Loss  0\n",
      "loop took 0.3030204772949219 seconds\n",
      "TIMESTEP 13 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.07272727272727274 / Q_MAX  0 / Loss  0\n",
      "loop took 0.34607505798339844 seconds\n",
      "TIMESTEP 14 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.3560476303100586 seconds\n",
      "TIMESTEP 15 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1272727272727273 / Q_MAX  0 / Loss  0\n",
      "loop took 0.5873804092407227 seconds\n",
      "TIMESTEP 16 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD -0.6470588235294118 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "loop took 0.22439789772033691 seconds\n",
      "TIMESTEP 17 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.04 / Q_MAX  0 / Loss  0\n",
      "loop took 0.27426671981811523 seconds\n",
      "TIMESTEP 18 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.05454545454545456 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2573127746582031 seconds\n",
      "TIMESTEP 19 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.07272727272727274 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2812492847442627 seconds\n",
      "TIMESTEP 20 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.09090909090909091 / Q_MAX  0 / Loss  0\n",
      "loop took 0.26822447776794434 seconds\n",
      "TIMESTEP 21 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.11818181818181818 / Q_MAX  0 / Loss  0\n",
      "loop took 0.36000943183898926 seconds\n",
      "TIMESTEP 22 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.13636363636363635 / Q_MAX  0 / Loss  0\n",
      "loop took 0.5744638442993164 seconds\n",
      "TIMESTEP 23 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD -0.6470588235294118 / Q_MAX  0 / Loss  0\n",
      "loop took 0.3399825096130371 seconds\n",
      "TIMESTEP 24 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.03636363636363637 / Q_MAX  0 / Loss  0\n",
      "loop took 0.3370969295501709 seconds\n",
      "TIMESTEP 25 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.06363636363636364 / Q_MAX  0 / Loss  0\n",
      "loop took 0.33415842056274414 seconds\n",
      "TIMESTEP 26 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.09090909090909091 / Q_MAX  0 / Loss  0\n",
      "loop took 0.3101189136505127 seconds\n",
      "TIMESTEP 27 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.11818181818181818 / Q_MAX  0 / Loss  0\n",
      "loop took 0.34108757972717285 seconds\n",
      "TIMESTEP 28 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.13636363636363635 / Q_MAX  0 / Loss  0\n",
      "loop took 0.3311131000518799 seconds\n",
      "TIMESTEP 29 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.16363636363636364 / Q_MAX  0 / Loss  0\n",
      "loop took 0.3310856819152832 seconds\n",
      "TIMESTEP 30 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.19090909090909092 / Q_MAX  0 / Loss  0\n",
      "loop took 0.3301568031311035 seconds\n",
      "TIMESTEP 31 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.21818181818181823 / Q_MAX  0 / Loss  0\n",
      "loop took 0.31914663314819336 seconds\n",
      "TIMESTEP 32 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.24545454545454548 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "loop took 0.2468101978302002 seconds\n",
      "TIMESTEP 33 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.3 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2483348846435547 seconds\n",
      "TIMESTEP 34 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.32 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2873072624206543 seconds\n",
      "TIMESTEP 35 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.30909090909090914 / Q_MAX  0 / Loss  0\n",
      "loop took 0.33011794090270996 seconds\n",
      "TIMESTEP 36 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.33636363636363636 / Q_MAX  0 / Loss  0\n",
      "loop took 0.5904757976531982 seconds\n",
      "TIMESTEP 37 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD -0.2894736842105263 / Q_MAX  0 / Loss  0\n",
      "loop took 0.3380892276763916 seconds\n",
      "TIMESTEP 38 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.03636363636363637 / Q_MAX  0 / Loss  0\n",
      "loop took 0.33011722564697266 seconds\n",
      "TIMESTEP 39 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.06363636363636364 / Q_MAX  0 / Loss  0\n",
      "loop took 0.3151562213897705 seconds\n",
      "TIMESTEP 40 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.09090909090909091 / Q_MAX  0 / Loss  0\n",
      "loop took 0.33504414558410645 seconds\n",
      "TIMESTEP 41 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.11818181818181818 / Q_MAX  0 / Loss  0\n",
      "loop took 0.32909440994262695 seconds\n",
      "TIMESTEP 42 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.13636363636363635 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2682836055755615 seconds\n",
      "TIMESTEP 43 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.18 / Q_MAX  0 / Loss  0\n",
      "loop took 0.262298583984375 seconds\n",
      "TIMESTEP 44 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.19090909090909092 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2533230781555176 seconds\n",
      "TIMESTEP 45 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.2090909090909091 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2433488368988037 seconds\n",
      "TIMESTEP 46 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.22727272727272727 / Q_MAX  0 / Loss  0\n",
      "loop took 0.21741890907287598 seconds\n",
      "TIMESTEP 47 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.27 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "loop took 0.19547772407531738 seconds\n",
      "TIMESTEP 48 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.29000000000000004 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2403566837310791 seconds\n",
      "TIMESTEP 49 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.31 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2483363151550293 seconds\n",
      "TIMESTEP 50 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.30000000000000004 / Q_MAX  0 / Loss  0\n",
      "loop took 0.24534320831298828 seconds\n",
      "TIMESTEP 51 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.3181818181818182 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2533230781555176 seconds\n",
      "TIMESTEP 52 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.33636363636363636 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2154242992401123 seconds\n",
      "TIMESTEP 53 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.39 / Q_MAX  0 / Loss  0\n",
      "loop took 0.5325803756713867 seconds\n",
      "TIMESTEP 54 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD -0.2682926829268293 / Q_MAX  0 / Loss  0\n",
      "loop took 0.3370940685272217 seconds\n",
      "TIMESTEP 55 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.03636363636363637 / Q_MAX  0 / Loss  0\n",
      "loop took 0.28523802757263184 seconds\n",
      "TIMESTEP 56 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.06363636363636364 / Q_MAX  0 / Loss  0\n",
      "loop took 0.3261291980743408 seconds\n",
      "TIMESTEP 57 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.08181818181818182 / Q_MAX  0 / Loss  0\n",
      "loop took 0.3321108818054199 seconds\n",
      "TIMESTEP 58 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.10909090909090911 / Q_MAX  0 / Loss  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop took 0.3311150074005127 seconds\n",
      "TIMESTEP 59 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.13636363636363635 / Q_MAX  0 / Loss  0\n",
      "loop took 0.33330702781677246 seconds\n",
      "TIMESTEP 60 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.16363636363636364 / Q_MAX  0 / Loss  0\n",
      "loop took 0.31595730781555176 seconds\n",
      "TIMESTEP 61 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.19090909090909092 / Q_MAX  0 / Loss  0\n",
      "loop took 0.354053258895874 seconds\n",
      "TIMESTEP 62 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.21818181818181823 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "loop took 0.26928138732910156 seconds\n",
      "TIMESTEP 63 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.27 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2582082748413086 seconds\n",
      "TIMESTEP 64 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.26363636363636367 / Q_MAX  0 / Loss  0\n",
      "loop took 0.28324270248413086 seconds\n",
      "TIMESTEP 65 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.2818181818181818 / Q_MAX  0 / Loss  0\n",
      "loop took 0.5714719295501709 seconds\n",
      "TIMESTEP 66 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD -0.3235294117647059 / Q_MAX  0 / Loss  0\n",
      "loop took 0.3281235694885254 seconds\n",
      "TIMESTEP 67 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.03636363636363637 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2738320827484131 seconds\n",
      "TIMESTEP 68 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.05454545454545456 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2927815914154053 seconds\n",
      "TIMESTEP 69 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.08181818181818182 / Q_MAX  0 / Loss  0\n",
      "loop took 0.28623437881469727 seconds\n",
      "TIMESTEP 70 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2622995376586914 seconds\n",
      "TIMESTEP 71 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1272727272727273 / Q_MAX  0 / Loss  0\n",
      "loop took 0.5336081981658936 seconds\n",
      "TIMESTEP 72 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD -0.6875 / Q_MAX  0 / Loss  0\n",
      "loop took 0.3370647430419922 seconds\n",
      "TIMESTEP 73 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.03636363636363637 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "loop took 0.26628804206848145 seconds\n",
      "TIMESTEP 74 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.07 / Q_MAX  0 / Loss  0\n",
      "loop took 0.3171513080596924 seconds\n",
      "TIMESTEP 75 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.08181818181818182 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "loop took 0.27837181091308594 seconds\n",
      "TIMESTEP 76 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.12000000000000002 / Q_MAX  0 / Loss  0\n",
      "loop took 0.33310842514038086 seconds\n",
      "TIMESTEP 77 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1272727272727273 / Q_MAX  0 / Loss  0\n",
      "loop took 0.36602044105529785 seconds\n",
      "TIMESTEP 78 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.15454545454545457 / Q_MAX  0 / Loss  0\n",
      "loop took 0.33104634284973145 seconds\n",
      "TIMESTEP 79 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.19090909090909092 / Q_MAX  0 / Loss  0\n",
      "loop took 0.28822898864746094 seconds\n",
      "TIMESTEP 80 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.2090909090909091 / Q_MAX  0 / Loss  0\n",
      "loop took 0.26628828048706055 seconds\n",
      "TIMESTEP 81 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.26 / Q_MAX  0 / Loss  0\n",
      "loop took 0.29121994972229004 seconds\n",
      "TIMESTEP 82 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.2545454545454546 / Q_MAX  0 / Loss  0\n",
      "loop took 0.31615519523620605 seconds\n",
      "TIMESTEP 83 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.2818181818181818 / Q_MAX  0 / Loss  0\n",
      "loop took 0.29813075065612793 seconds\n",
      "TIMESTEP 84 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.30000000000000004 / Q_MAX  0 / Loss  0\n",
      "loop took 0.31316304206848145 seconds\n",
      "TIMESTEP 85 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.32727272727272727 / Q_MAX  0 / Loss  0\n",
      "loop took 0.30710339546203613 seconds\n",
      "TIMESTEP 86 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.35454545454545455 / Q_MAX  0 / Loss  0\n",
      "loop took 0.3143479824066162 seconds\n",
      "TIMESTEP 87 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.38181818181818183 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "loop took 0.21741819381713867 seconds\n",
      "TIMESTEP 88 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.44000000000000006 / Q_MAX  0 / Loss  0\n",
      "loop took 0.5326120853424072 seconds\n",
      "TIMESTEP 89 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD -0.24444444444444444 / Q_MAX  0 / Loss  0\n",
      "loop took 0.6244239807128906 seconds\n",
      "TIMESTEP 90 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.06363636363636364 / Q_MAX  0 / Loss  0\n",
      "loop took 0.26421356201171875 seconds\n",
      "TIMESTEP 91 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.08181818181818182 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2862358093261719 seconds\n",
      "TIMESTEP 92 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
      "loop took 0.3390312194824219 seconds\n",
      "TIMESTEP 93 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1272727272727273 / Q_MAX  0 / Loss  0\n",
      "loop took 0.33011722564697266 seconds\n",
      "TIMESTEP 94 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.15454545454545457 / Q_MAX  0 / Loss  0\n",
      "loop took 0.3151583671569824 seconds\n",
      "TIMESTEP 95 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.2 / Q_MAX  0 / Loss  0\n",
      "loop took 0.3103032112121582 seconds\n",
      "TIMESTEP 96 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.2090909090909091 / Q_MAX  0 / Loss  0\n",
      "loop took 0.34102535247802734 seconds\n",
      "TIMESTEP 97 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.23636363636363636 / Q_MAX  0 / Loss  0\n",
      "loop took 0.33211278915405273 seconds\n",
      "TIMESTEP 98 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.2545454545454546 / Q_MAX  0 / Loss  0\n",
      "loop took 0.3300464153289795 seconds\n",
      "TIMESTEP 99 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.2818181818181818 / Q_MAX  0 / Loss  0\n",
      "loop took 0.3341066837310791 seconds\n",
      "TIMESTEP 100 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.30909090909090914 / Q_MAX  0 / Loss  0\n",
      "loop took 0.3256850242614746 seconds\n",
      "TIMESTEP 101 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.33636363636363636 / Q_MAX  0 / Loss  0\n",
      "loop took 0.5854830741882324 seconds\n",
      "TIMESTEP 102 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD -0.28205128205128205 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2691619396209717 seconds\n",
      "TIMESTEP 103 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.03636363636363637 / Q_MAX  0 / Loss  0\n",
      "loop took 0.2782154083251953 seconds\n",
      "TIMESTEP 104 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.05454545454545456 / Q_MAX  0 / Loss  0\n",
      "loop took 0.3392293453216553 seconds\n",
      "TIMESTEP 105 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.08181818181818182 / Q_MAX  0 / Loss  0\n",
      "loop took 0.32399606704711914 seconds\n",
      "TIMESTEP 106 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.10909090909090911 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "loop took 0.25831007957458496 seconds\n",
      "TIMESTEP 107 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.14 / Q_MAX  0 / Loss  0\n",
      "loop took 0.5904242992401123 seconds\n",
      "TIMESTEP 108 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD -0.6470588235294118 / Q_MAX  0 / Loss  0\n",
      "loop took 0.3021886348724365 seconds\n",
      "TIMESTEP 109 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.03636363636363637 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "loop took 0.206406831741333 seconds\n",
      "TIMESTEP 110 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.06000000000000001 / Q_MAX  0 / Loss  0\n",
      "loop took 0.23630785942077637 seconds\n",
      "TIMESTEP 111 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.07272727272727274 / Q_MAX  0 / Loss  0\n",
      "loop took 2.265949010848999 seconds\n",
      "TIMESTEP 112 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.09090909090909091 / Q_MAX  0 / Loss  0\n",
      "loop took 0.33245134353637695 seconds\n",
      "TIMESTEP 113 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.09090909090909091 / Q_MAX  0 / Loss  0\n",
      "loop took 0.28623509407043457 seconds\n",
      "TIMESTEP 114 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.11818181818181818 / Q_MAX  0 / Loss  0\n",
      "loop took 0.31607651710510254 seconds\n",
      "TIMESTEP 115 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.13636363636363635 / Q_MAX  0 / Loss  0\n",
      "loop took 0.3317751884460449 seconds\n",
      "TIMESTEP 116 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.16363636363636364 / Q_MAX  0 / Loss  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop took 0.42985105514526367 seconds\n",
      "TIMESTEP 117 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.19090909090909092 / Q_MAX  0 / Loss  0\n"
     ]
    },
    {
     "ename": "ElementNotInteractableException",
     "evalue": "Message: element not interactable\n  (Session info: chrome=75.0.3770.142)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mElementNotInteractableException\u001b[0m           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-cced8fd36f13>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0minit_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplayGame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobserve\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-25fbf9051fdb>\u001b[0m in \u001b[0;36mplayGame\u001b[1;34m(observe)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuildmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mtrainNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgame_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mobserve\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobserve\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-26ecc14d59d7>\u001b[0m in \u001b[0;36mtrainNetwork\u001b[1;34m(model, game_state, observe)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;31m#run the selected action and observed next state and reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         \u001b[0mx_t1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgame_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loop took {} seconds'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlast_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# helpful for measuring frame rate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[0mlast_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-c19911624dbb>\u001b[0m in \u001b[0;36mget_state\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mis_over\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;31m#game over\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m             \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscreen_capture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-ac1ae6458163>\u001b[0m in \u001b[0;36mjump\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_game\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrashed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mjump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_game\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpress_up\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mduck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_game\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpress_down\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-1b170e6b0673>\u001b[0m in \u001b[0;36mpress_up\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     34\u001b[0m                         \u001b[1;31m# skip learning at this time and make the model wait\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpress_up\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_driver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_tag_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"body\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mARROW_UP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mscore_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_driver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute_script\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"return Runner.instance_.distanceMeter.digits\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py\u001b[0m in \u001b[0;36msend_keys\u001b[1;34m(self, *value)\u001b[0m\n\u001b[0;32m    477\u001b[0m         self._execute(Command.SEND_KEYS_TO_ELEMENT,\n\u001b[0;32m    478\u001b[0m                       {'text': \"\".join(keys_to_typing(value)),\n\u001b[1;32m--> 479\u001b[1;33m                        'value': keys_to_typing(value)})\n\u001b[0m\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# RenderedWebElement Items\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py\u001b[0m in \u001b[0;36m_execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    631\u001b[0m             \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 633\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    634\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfind_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[0;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mElementNotInteractableException\u001b[0m: Message: element not interactable\n  (Session info: chrome=75.0.3770.142)\n"
     ]
    }
   ],
   "source": [
    "init_cache()\n",
    "playGame(observe=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
